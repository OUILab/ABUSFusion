{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f46a25-f081-445a-b8d8-aaf3f9f8aa8d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # 3D Ultrasound Volume Reconstruction with DCL-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66422217-b532-4f39-b8db-34eb617bb3ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/home/varun/xia_lab/repos/ABUSFusion/notebooks/new.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmplot3d\u001b[39;00m \u001b[39mimport\u001b[39;00m Axes3D\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m efficientnet_b1\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m zoom\n\u001b[1;32m     15\u001b[0m \u001b[39m# Import custom modules\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import efficientnet_b1\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Import custom modules\n",
    "from freehand.network import build_model\n",
    "from freehand.transform import (\n",
    "    LabelTransform,\n",
    "    PredictionTransform,\n",
    "    TransformAccumulation,\n",
    ")\n",
    "from freehand.utils import pair_samples, reference_image_points, type_dim\n",
    "from freehand.loss import PointDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1a988-d3b4-4b29-a144-1838fcb44919",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19982c01-8eb4-4ef3-bce7-d1abfd449af8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, scan_name, num_samples=10, sample_range=10):\n",
    "        self.data_dir = data_dir\n",
    "        self.scan_name = scan_name\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_range = sample_range\n",
    "\n",
    "        self.frames = np.load(f\"{data_dir}{scan_name}/{scan_name}_frames.npy\")\n",
    "        self.tracker_data = np.loadtxt(f\"{data_dir}{scan_name}/{scan_name}_pos.txt\")\n",
    "\n",
    "        # Ensure frames are in (C, H, W) format\n",
    "        if self.frames.ndim == 3:\n",
    "            self.frames = np.expand_dims(self.frames, axis=0)\n",
    "        elif self.frames.ndim == 4 and self.frames.shape[-1] in [1, 3]:\n",
    "            self.frames = np.moveaxis(self.frames, -1, 0)\n",
    "\n",
    "        # Normalize frames\n",
    "        self.frames = self.frames.astype(np.float32) / 255.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracker_data) - self.sample_range + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = torch.tensor(\n",
    "            self.frames[:, idx : idx + self.sample_range], dtype=torch.float32\n",
    "        )\n",
    "        tracker_data = torch.tensor(\n",
    "            self.tracker_data[idx : idx + self.sample_range, 2:], dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return frames, tracker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6303a-3e4a-4d01-9d32-8a4f68d2d276",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "SCAN_NAME = \"phantom\"\n",
    "dataset = UltrasoundDataset(DATA_DIR, SCAN_NAME)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53461b-dece-437e-8d9c-98a73f133470",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c9733-db74-403e-8da1-2eb00543ecca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_PRED = 9\n",
    "PRED_TYPE = \"parameter\"\n",
    "LABEL_TYPE = \"point\"\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Create model\n",
    "frame_size = dataset.frames.shape[2:]\n",
    "image_points = reference_image_points(frame_size, 2).to(device)\n",
    "data_pairs = pair_samples(NUM_SAMPLES, NUM_PRED).to(device)\n",
    "pred_dim = type_dim(PRED_TYPE, image_points.shape[1], data_pairs.shape[0])\n",
    "\n",
    "model = build_model(efficientnet_b1, in_frames=NUM_SAMPLES, out_dim=pred_dim).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d180d-8aca-467b-8056-7d45fe089f0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (frames, tracker_data) in enumerate(dataloader):\n",
    "            frames, tracker_data = frames.to(device), tracker_data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "\n",
    "            # Compute relative transformations\n",
    "            relative_transformations = tracker_data[:, 1:] - tracker_data[:, :-1]\n",
    "            relative_transformations = relative_transformations.reshape(outputs.shape)\n",
    "\n",
    "            loss = criterion(outputs, relative_transformations)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf09e1-64db-46a1-ba07-9dcf08009b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "trained_model = train_model(model, dataloader, criterion, optimizer, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e098a-899b-4d0a-b08d-51911eebb947",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 3D Volume Reconstruction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e4f19-b8f1-454e-9f3f-f4244e502411",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def reconstruct_volume(model, dataset, device, num_frames=100):\n",
    "    model.eval()\n",
    "    volume = []\n",
    "    current_position = torch.zeros(1, 3).to(device)\n",
    "    current_orientation = torch.zeros(1, 4).to(device)  # Using quaternions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(\n",
    "            0,\n",
    "            min(len(dataset), num_frames - dataset.num_samples + 1),\n",
    "            dataset.num_samples - 1,\n",
    "        ):\n",
    "            frames, _ = dataset[i]\n",
    "            frames = frames.unsqueeze(0).to(device)\n",
    "            predicted_params = model(frames)\n",
    "\n",
    "            # Accumulate transformations\n",
    "            current_position += predicted_params[0, :3]\n",
    "            current_orientation += predicted_params[0, 3:]\n",
    "\n",
    "            # Add the middle frame of the sequence to the volume\n",
    "            middle_frame = frames[0, :, dataset.num_samples // 2].cpu().numpy()\n",
    "            volume.append(\n",
    "                (\n",
    "                    middle_frame,\n",
    "                    current_position.cpu().numpy(),\n",
    "                    current_orientation.cpu().numpy(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af20ed-434b-414d-b594-004b4600652e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def visualize_3d_volume(volume):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for i, (frame, position, orientation) in enumerate(volume):\n",
    "        # Create a plane representing the ultrasound frame\n",
    "        X, Y = np.meshgrid(range(frame.shape[1]), range(frame.shape[0]))\n",
    "        Z = np.full_like(X, i)\n",
    "\n",
    "        # Apply transformations (simplified - you might want to use proper rotation matrices)\n",
    "        X_trans = X + position[0, 0]\n",
    "        Y_trans = Y + position[0, 1]\n",
    "        Z_trans = Z + position[0, 2]\n",
    "\n",
    "        # Plot the transformed frame\n",
    "        ax.plot_surface(\n",
    "            X_trans, Y_trans, Z_trans, facecolors=plt.cm.gray(frame[0]), shade=False\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_title(\"Reconstructed 3D Ultrasound Volume\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400c33f-067f-44ba-bfde-32fa94002694",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "reconstructed_volume = reconstruct_volume(trained_model, dataset, device)\n",
    "visualize_3d_volume(reconstructed_volume)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
