{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de8d981-9501-4daf-8949-515e8727aea8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # DCL-Net Training for Sensorless Freehand 3D Ultrasound Reconstruction\n",
    "\n",
    " This notebook implements the training process for DCL-Net as described in the paper. We'll go through the following steps:\n",
    " 1. Import required libraries\n",
    " 2. Read and preprocess data\n",
    " 3. Define the DCL-Net model\n",
    " 4. Implement the loss function\n",
    " 5. Set up the training loop\n",
    " 6. Train the model\n",
    " 7. Evaluate the model\n",
    " 8. Reconstruct the 3D volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c962b-ee8a-4708-b59e-a03425f57ea5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf51a9-77d4-4f4a-9f80-116e5938fbad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/home/varun/xia_lab/repos/ABUSFusion/notebooks/dclnet.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mSimpleITK\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msitk\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e412-0a7f-4bc5-8f95-a7a2bfe7e31f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # DCL-Net Training for Sensorless Freehand 3D Ultrasound Reconstruction\n",
    "\n",
    " This notebook implements the training process for DCL-Net as described in the paper. We'll go through the following steps:\n",
    " 1. Import required libraries\n",
    " 2. Read and preprocess data\n",
    " 3. Define the DCL-Net model\n",
    " 4. Implement the loss function\n",
    " 5. Set up the training loop\n",
    " 6. Train the model\n",
    " 7. Evaluate the model\n",
    " 8. Reconstruct the 3D volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13290230-4747-4875-a73e-39ab8a682a70",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50968d75-e6ce-447d-80d3-e18b76e9c15a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mSimpleITK\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msitk\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fd29b-ef71-4533-a204-49017b256b76",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 2. Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbaed1-61ff-4cd6-9d59-f83c2c2e84a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class USDataset(Dataset):\n",
    "    def __init__(self, video_path, imu_csv_path, tracker_csv_path, num_frames=5):\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Read video\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Read IMU data\n",
    "        self.imu_data = pd.read_csv(imu_csv_path)\n",
    "\n",
    "        # Read tracker data\n",
    "        self.tracker_data = pd.read_csv(tracker_csv_path)\n",
    "\n",
    "        # Synchronize data (assuming timestamps are available)\n",
    "        self.sync_data()\n",
    "\n",
    "    def sync_data(self):\n",
    "        # Implement synchronization logic here\n",
    "        # This should align video frames with IMU and tracker data\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_frames - self.num_frames + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = []\n",
    "        for i in range(self.num_frames):\n",
    "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, idx + i)\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frames.append(frame)\n",
    "\n",
    "        frames = np.stack(frames, axis=0)\n",
    "\n",
    "        # Get corresponding tracker data\n",
    "        tracker_data = self.tracker_data.iloc[idx : idx + self.num_frames]\n",
    "\n",
    "        # Calculate mean transformation parameters\n",
    "        mean_params = self.calculate_mean_params(tracker_data)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(frames).float().unsqueeze(0),\n",
    "            torch.from_numpy(mean_params).float(),\n",
    "        )\n",
    "\n",
    "    def calculate_mean_params(self, tracker_data):\n",
    "        # Implement logic to calculate mean transformation parameters\n",
    "        # This should return a 6-element array (tx, ty, tz, rx, ry, rz)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7647b5-ea77-4fa7-83d7-855fff973432",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 3. Define the DCL-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f1983-f0d1-45a1-a680-07f966c27a57",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class DCLNet(nn.Module):\n",
    "    def __init__(self, num_frames=5, cardinality=32):\n",
    "        super(DCLNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            1,\n",
    "            64,\n",
    "            kernel_size=(3, 7, 7),\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(1, 3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(\n",
    "            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)\n",
    "        )\n",
    "\n",
    "        # Add ResNeXt blocks here\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(2048, 6)  # Output 6 parameters (tx, ty, tz, rx, ry, rz)\n",
    "\n",
    "        # Add attention module\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.BatchNorm3d(2048),\n",
    "            nn.Conv3d(2048, 1024, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(1024, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Add ResNeXt block operations here\n",
    "\n",
    "        attention = self.attention(x)\n",
    "        x = x * attention\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b695ed-3913-4fe0-b763-22c29ef7080e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 4. Implement the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1182c-cece-4bb2-af63-404cef503a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class MSECorrelationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(MSECorrelationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        mse_loss = self.mse(pred, target)\n",
    "\n",
    "        # Calculate correlation loss\n",
    "        pred_mean = pred.mean(dim=0, keepdim=True)\n",
    "        target_mean = target.mean(dim=0, keepdim=True)\n",
    "        pred_std = pred.std(dim=0, unbiased=False)\n",
    "        target_std = target.std(dim=0, unbiased=False)\n",
    "\n",
    "        correlation = ((pred - pred_mean) * (target - target_mean)).mean(dim=0) / (\n",
    "            pred_std * target_std\n",
    "        )\n",
    "        correlation_loss = 1 - correlation.mean()\n",
    "\n",
    "        return self.alpha * mse_loss + (1 - self.alpha) * correlation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2009d-20cf-4d52-a8bf-6a24445f44ea",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 5. Set up the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d43ad-f064-4087-a989-6274f3ba4ea1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frames, labels in val_loader:\n",
    "                frames, labels = frames.to(device), labels.to(device)\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9ecd9-e00a-4267-8845-89b6afdc2ac3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0994970-4d85-42a5-a4e2-4b84dbf78f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Set up dataset and data loaders\n",
    "dataset = USDataset(\n",
    "    \"path/to/video.mp4\", \"path/to/imu_data.csv\", \"path/to/tracker_data.csv\"\n",
    ")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DCLNet()\n",
    "criterion = MSECorrelationLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 100\n",
    "trained_model = train(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, device\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), \"dcl_net_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473946d-e020-4cd8-9282-5286300b9cf8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 7. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97455056-b6d6-432e-baea-35c238c24677",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mean_error = np.mean(np.abs(all_predictions - all_labels), axis=0)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mean_error}\")\n",
    "\n",
    "    return all_predictions, all_labels\n",
    "\n",
    "\n",
    "# Set up test dataset and loader\n",
    "test_dataset = USDataset(\n",
    "    \"path/to/test_video.mp4\",\n",
    "    \"path/to/test_imu_data.csv\",\n",
    "    \"path/to/test_tracker_data.csv\",\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load the trained model\n",
    "model = DCLNet()\n",
    "model.load_state_dict(torch.load(\"dcl_net_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "criterion = MSECorrelationLoss()\n",
    "predictions, labels = evaluate(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa356a75-ac3e-45db-8cd0-4e66f2f33db3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 8. Reconstruct the 3D volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e13d0-270a-4bc3-a126-e62fd84f4795",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def reconstruct_volume(\n",
    "    model, video_path, imu_csv_path, tracker_csv_path, output_path, device\n",
    "):\n",
    "    # Set up dataset for the entire video\n",
    "    dataset = USDataset(video_path, imu_csv_path, tracker_csv_path)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    transformations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, _ in tqdm(data_loader, desc=\"Predicting transformations\"):\n",
    "            frames = frames.to(device)\n",
    "            output = model(frames)\n",
    "            transformations.append(output.cpu().numpy()[0])\n",
    "\n",
    "    transformations = np.array(transformations)\n",
    "\n",
    "    # Reconstruct 3D volume using predicted transformations\n",
    "    volume = reconstruct_3d_volume(dataset, transformations)\n",
    "\n",
    "    # Save the reconstructed volume\n",
    "    sitk_volume = sitk.GetImageFromArray(volume)\n",
    "    sitk.WriteImage(sitk_volume, output_path)\n",
    "\n",
    "    return volume\n",
    "\n",
    "\n",
    "def reconstruct_3d_volume(dataset, transformations):\n",
    "    # Initialize an empty volume\n",
    "    volume = np.zeros((256, 256, len(transformations)), dtype=np.uint8)\n",
    "\n",
    "    # Set up initial transformation matrix\n",
    "    current_transform = np.eye(4)\n",
    "\n",
    "    for i, transformation in enumerate(transformations):\n",
    "        # Update current transformation\n",
    "        delta_transform = transformation_vector_to_matrix(transformation)\n",
    "        current_transform = np.dot(current_transform, delta_transform)\n",
    "\n",
    "        # Get the current frame\n",
    "        frame = dataset[i][0].numpy()[0, 0]\n",
    "\n",
    "        # Apply transformation to the frame and insert it into the volume\n",
    "        transformed_frame = apply_transformation(frame, current_transform)\n",
    "        volume[:, :, i] = transformed_frame\n",
    "\n",
    "    return volume\n",
    "\n",
    "\n",
    "def transformation_vector_to_matrix(vector):\n",
    "    # Convert 6-element vector (tx, ty, tz, rx, ry, rz) to 4x4 transformation matrix\n",
    "    tx, ty, tz, rx, ry, rz = vector\n",
    "\n",
    "    # Create rotation matrix\n",
    "    Rx = np.array(\n",
    "        [[1, 0, 0], [0, np.cos(rx), -np.sin(rx)], [0, np.sin(rx), np.cos(rx)]]\n",
    "    )\n",
    "    Ry = np.array(\n",
    "        [[np.cos(ry), 0, np.sin(ry)], [0, 1, 0], [-np.sin(ry), 0, np.cos(ry)]]\n",
    "    )\n",
    "    Rz = np.array(\n",
    "        [[np.cos(rz), -np.sin(rz), 0], [np.sin(rz), np.cos(rz), 0], [0, 0, 1]]\n",
    "    )\n",
    "    R = np.dot(Rz, np.dot(Ry, Rx))\n",
    "\n",
    "    # Create translation vector\n",
    "    T = np.array([tx, ty, tz])\n",
    "\n",
    "    # Combine rotation and translation into 4x4 matrix\n",
    "    M = np.eye(4)\n",
    "    M[:3, :3] = R\n",
    "    M[:3, 3] = T\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def apply_transformation(frame, transform):\n",
    "    # Apply 3D transformation to a 2D frame\n",
    "    height, width = frame.shape\n",
    "    y, x = np.meshgrid(np.arange(height), np.arange(width), indexing=\"ij\")\n",
    "    homogeneous_coords = np.stack(\n",
    "        [\n",
    "            x.flatten(),\n",
    "            y.flatten(),\n",
    "            np.zeros_like(x.flatten()),\n",
    "            np.ones_like(x.flatten()),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    transformed_coords = np.dot(transform, homogeneous_coords.T).T\n",
    "    transformed_coords = transformed_coords[:, :2] / transformed_coords[:, 3:]\n",
    "\n",
    "    transformed_frame = cv2.remap(\n",
    "        frame,\n",
    "        transformed_coords[:, 0].reshape(height, width).astype(np.float32),\n",
    "        transformed_coords[:, 1].reshape(height, width).astype(np.float32),\n",
    "        cv2.INTER_LINEAR,\n",
    "    )\n",
    "\n",
    "    return transformed_frame\n",
    "\n",
    "\n",
    "# Reconstruct 3D volume\n",
    "reconstructed_volume = reconstruct_volume(\n",
    "    model,\n",
    "    \"path/to/test_video.mp4\",\n",
    "    \"path/to/test_imu_data.csv\",\n",
    "    \"path/to/test_tracker_data.csv\",\n",
    "    \"reconstructed_volume.nii.gz\",\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36058234-5a3e-4051-b313-f781b7df89f2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 9. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41e368-ada0-404c-b2de-cfb103ac10a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def visualize_results(predictions, labels, reconstructed_volume):\n",
    "    # Plot predicted vs. ground truth transformation parameters\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    param_names = [\"tx\", \"ty\", \"tz\", \"rx\", \"ry\", \"rz\"]\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.scatter(labels[:, i], predictions[:, i], alpha=0.5)\n",
    "        ax.plot(\n",
    "            [labels[:, i].min(), labels[:, i].max()],\n",
    "            [labels[:, i].min(), labels[:, i].max()],\n",
    "            \"r--\",\n",
    "            lw=2,\n",
    "        )\n",
    "        ax.set_xlabel(f\"Ground Truth {param_names[i]}\")\n",
    "        ax.set_ylabel(f\"Predicted {param_names[i]}\")\n",
    "        ax.set_title(f\"{param_names[i]} Prediction\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize reconstructed volume\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(\n",
    "        reconstructed_volume[:, :, reconstructed_volume.shape[2] // 2], cmap=\"gray\"\n",
    "    )\n",
    "    axs[0].set_title(\"Axial View\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(\n",
    "        reconstructed_volume[:, reconstructed_volume.shape[1] // 2, :], cmap=\"gray\"\n",
    "    )\n",
    "    axs[1].set_title(\"Coronal View\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(\n",
    "        reconstructed_volume[reconstructed_volume.shape[0] // 2, :, :], cmap=\"gray\"\n",
    "    )\n",
    "    axs[2].set_title(\"Sagittal View\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "visualize_results(predictions, labels, reconstructed_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917a3a8-e6b2-4b93-a3fd-737e9bcddd08",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 10. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2f410-fcb2-4d08-866c-816465796bed",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " In this notebook, we have implemented the DCL-Net for sensorless freehand 3D ultrasound reconstruction. We have covered the following steps:\n",
    "\n",
    " 1. Data preprocessing and loading\n",
    " 2. Model architecture implementation\n",
    " 3. Custom loss function (MSE + Correlation Loss)\n",
    " 4. Training and evaluation\n",
    " 5. 3D volume reconstruction\n",
    " 6. Results visualization\n",
    "\n",
    " The DCL-Net shows promising results in estimating the transformation parameters between consecutive ultrasound frames. However, there are several areas for potential improvement and future work:\n",
    "\n",
    " 1. Fine-tuning hyperparameters: Experiment with different learning rates, batch sizes, and model architectures to optimize performance.\n",
    " 2. Data augmentation: Implement more advanced data augmentation techniques to improve model generalization.\n",
    " 3. Attention mechanism: Further refine the attention module to focus on the most informative regions of the ultrasound frames.\n",
    " 4. Multi-task learning: Explore the possibility of jointly predicting transformation parameters and segmenting anatomical structures.\n",
    " 5. Real-time reconstruction: Optimize the model and reconstruction pipeline for real-time 3D volume generation during freehand ultrasound scanning.\n",
    " 6. Clinical validation: Conduct extensive clinical validation studies to assess the accuracy and reliability of the reconstructed 3D volumes in various anatomical regions and scanning scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d24c0c-f0f1-4395-914d-cc520e31b140",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 11. Save and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e277cfb-ba05-4843-bf4d-127c20ea7dd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Save predictions and labels\n",
    "np.save(\"predictions.npy\", predictions)\n",
    "np.save(\"labels.npy\", labels)\n",
    "\n",
    "# Export visualizations\n",
    "plt.savefig(\"transformation_predictions.png\")\n",
    "plt.savefig(\"reconstructed_volume_views.png\")\n",
    "\n",
    "print(\"Results saved and exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
