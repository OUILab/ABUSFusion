{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59741aac-6342-4da5-bb10-3782a1c8c03d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Ultrasound Reconstruction with IMU and Optical Tracker Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e65153-05a0-4c6f-bf1d-be012c384324",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d89fb6-dff5-4096-a5de-b13b07c05133",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mUntitled-4:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmplot3d\u001b[39;00m \u001b[39mimport\u001b[39;00m Axes3D\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m RegularGridInterpolator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy.spatial.transform import Rotation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e157830-9405-4201-8dad-de0bfa80927d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b257f5b-cd1c-4ca9-a6c9-28271fd6d2fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_hdf(file_path)\n",
    "\n",
    "\n",
    "file_path = \"/home/varun/xia_lab/repos/ABUSFusion/scans/20240826/wrist_data.h5\"\n",
    "df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77fbf2-ad3c-4a13-9798-1bb25b3014f0",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cde6b-9b08-4bd0-bcbf-2b7983d57fe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class UltrasoundSequenceDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length=2, transform=None, downsample_factor=3):\n",
    "        self.df = df\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "        # Create a resizing transform\n",
    "        self.resize = transforms.Resize(\n",
    "            (1000 // self.downsample_factor, 657 // self.downsample_factor)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx : idx + self.sequence_length]\n",
    "\n",
    "        frames = torch.tensor(np.stack(sequence[\"frame\"].values)).float()\n",
    "        frames = frames.mean(dim=-1, keepdim=True)  # Convert RGB to grayscale\n",
    "        frames = frames.permute(\n",
    "            0, 3, 1, 2\n",
    "        )  # Change to (sequence_length, channels, height, width)\n",
    "\n",
    "        # Apply resizing to each frame in the sequence\n",
    "        resized_frames = torch.stack([self.resize(frame) for frame in frames])\n",
    "\n",
    "        imu_data = torch.tensor(\n",
    "            sequence[\n",
    "                [\n",
    "                    \"imu_acc_x\",\n",
    "                    \"imu_acc_y\",\n",
    "                    \"imu_acc_z\",\n",
    "                    \"imu_orientation_x\",\n",
    "                    \"imu_orientation_y\",\n",
    "                    \"imu_orientation_z\",\n",
    "                ]\n",
    "            ].values\n",
    "        ).float()\n",
    "\n",
    "        ot_data = torch.tensor(\n",
    "            sequence[\n",
    "                [\"ot_pos_x\", \"ot_pos_y\", \"ot_pos_z\", \"ot_qw\", \"ot_qx\", \"ot_qy\", \"ot_qz\"]\n",
    "            ].values\n",
    "        ).float()\n",
    "\n",
    "        sample = (resized_frames, imu_data, ot_data)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347cce2-b9bc-4162-80cf-ef5eab8af205",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class QuaternionToEulerTransform:\n",
    "    def __call__(self, sample):\n",
    "        frames, imu_data, ot_data = sample\n",
    "\n",
    "        ot_pos = ot_data[..., :3]\n",
    "        ot_quat = ot_data[..., 3:]\n",
    "\n",
    "        # Convert quaternions to Euler angles\n",
    "        ot_euler = torch.tensor(Rotation.from_quat(ot_quat.numpy()).as_euler('xyz')).float()\n",
    "        target = torch.cat([ot_pos, ot_euler], dim=-1)\n",
    "\n",
    "        return frames, imu_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbd361-1bca-4446-8b29-389d6121f68a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "transform = QuaternionToEulerTransform()\n",
    "dataset = UltrasoundSequenceDataset(\n",
    "    df, sequence_length=2, transform=transform, downsample_factor=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63a9b3-4ae9-49d7-a47f-0f890a738f2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Split the dataset\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(dataset)), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66513fab-d03b-4839-b7d4-4252a14b3084",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b0709-ed8b-446a-adab-9dca2ae59299",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=1, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e654be2-1177-4ca8-8f7d-5099e1da0df2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 4. Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09503847-c072-4459-a667-59d3463e5750",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class UltrasoundIMUModel(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_height=1000, input_width=657):\n",
    "        super(UltrasoundIMUModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        conv_output_height = input_height // 4\n",
    "        conv_output_width = input_width // 4\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            64 * conv_output_height * conv_output_width + 6, 128, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 6)  # Output: tx, ty, tz, rx, ry, rz\n",
    "\n",
    "    def forward(self, frames, imu_data):\n",
    "        batch_size, seq_len, _, height, width = frames.shape\n",
    "\n",
    "        frame_features = []\n",
    "        for i in range(seq_len):\n",
    "            x = self.pool(torch.relu(self.conv1(frames[:, i, :, :, :])))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            frame_features.append(x.view(batch_size, -1))\n",
    "\n",
    "        combined_features = torch.cat(\n",
    "            [torch.stack(frame_features, dim=1), imu_data], dim=2\n",
    "        )\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined_features)\n",
    "        transformations = self.fc(lstm_out[:, -1, :])  # Use only the last output\n",
    "\n",
    "        return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0e314-41b5-4367-9c1b-5eba22704163",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Initialize model\n",
    "model = UltrasoundIMUModel(\n",
    "    input_channels=1, input_height=1000 // 3, input_width=657 // 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26f76-67e2-459d-a93b-3a972bb0335d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a4da6-a4d9-4db9-9169-7660fa15b9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3639f-5cb9-4234-87dd-866352fd89d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=100,\n",
    "    patience=10,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_weights = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        epoch_progress = tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", position=1, leave=False\n",
    "        )\n",
    "        for frames, imu_data, targets in epoch_progress:\n",
    "            frames, imu_data, targets = (\n",
    "                frames.to(device),\n",
    "                imu_data.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames, imu_data)\n",
    "            loss = criterion(outputs, targets[:, -1, :])  # Compare with the last target in the sequence\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loss_avg = train_loss / (epoch_progress.n + 1)\n",
    "\n",
    "            epoch_progress.set_postfix({\"Avg Loss\": f\"{train_loss_avg:.4f}\"})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for frames, imu_data, targets in val_loader:\n",
    "                frames, imu_data, targets = (\n",
    "                    frames.to(device),\n",
    "                    imu_data.to(device),\n",
    "                    targets.to(device),\n",
    "                )\n",
    "                outputs = model(frames, imu_data)\n",
    "                loss = criterion(outputs, targets[:, -1, :])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                tqdm.write(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df9f68-84c9-45ea-b41c-387df572d529",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler\n",
    ")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(trained_model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869caa5f-8713-4a40-862d-5068db523da3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2ebed-5824-4a92-93e5-528be227ce76",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, imu_data, targets in data_loader:\n",
    "            frames, imu_data, targets = (\n",
    "                frames.to(device),\n",
    "                imu_data.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "            outputs = model(frames, imu_data)\n",
    "            loss = criterion(outputs, targets[:, -1, :])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets[:, -1, :].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    return avg_loss, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadedd9c-4aa8-435e-b753-49f8abfd84ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_predictions, val_targets = evaluate_model(trained_model, val_loader)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad77fa6-66ad-4727-bef0-fb8979671595",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ecc52-2841-4749-b828-f612c0da8883",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def visualize_predictions(predictions, targets):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    titles = [\n",
    "        \"Translation X\",\n",
    "        \"Translation Y\",\n",
    "        \"Translation Z\",\n",
    "        \"Rotation X\",\n",
    "        \"Rotation Y\",\n",
    "        \"Rotation Z\",\n",
    "    ]\n",
    "\n",
    "    for i in range(6):\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        ax.plot(predictions[:100, i], label=\"Predicted\")\n",
    "        ax.plot(targets[:100, i], label=\"Ground Truth\")\n",
    "        ax.set_title(titles[i])\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b278a5-c34f-4bb7-9772-968043083c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "visualize_predictions(val_predictions, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5dfca-85bf-46ef-ba00-5e7f3b67b4af",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 8. Generate ground truth volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc24071-2c2c-4cb2-9ca7-8ca46d16f4b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def plot_trajectory_and_poses(df, frame_stride=1, scale=10):\n",
    "    # Extract optical tracker data\n",
    "    positions = df[[\"ot_pos_x\", \"ot_pos_y\", \"ot_pos_z\"]].values\n",
    "    orientations = df[[\"ot_qw\", \"ot_qx\", \"ot_qy\", \"ot_qz\"]].values\n",
    "\n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Plot trajectory\n",
    "    ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], \"b-\", label=\"Trajectory\")\n",
    "\n",
    "    # Plot pose vectors\n",
    "    for i in range(0, len(df), frame_stride):\n",
    "        pos = positions[i]\n",
    "        quat = orientations[i]\n",
    "\n",
    "        # Convert quaternion to rotation matrix\n",
    "        rot_matrix = Rotation.from_quat(quat).as_matrix()\n",
    "\n",
    "        # Define coordinate axes\n",
    "        axes = np.array([[scale, 0, 0], [0, scale, 0], [0, 0, scale]])\n",
    "\n",
    "        # Rotate and translate axes\n",
    "        transformed_axes = rot_matrix @ axes.T + pos[:, np.newaxis]\n",
    "\n",
    "        # Plot coordinate axes\n",
    "        colors = [\"r\", \"g\", \"b\"]\n",
    "        labels = [\"X\", \"Y\", \"Z\"]\n",
    "        for j, (color, label) in enumerate(zip(colors, labels)):\n",
    "            if i == 0:  # Only add label for the first iteration\n",
    "                ax.quiver(\n",
    "                    pos[0],\n",
    "                    pos[1],\n",
    "                    pos[2],\n",
    "                    transformed_axes[0, j] - pos[0],\n",
    "                    transformed_axes[1, j] - pos[1],\n",
    "                    transformed_axes[2, j] - pos[2],\n",
    "                    color=color,\n",
    "                    label=label,\n",
    "                )\n",
    "            else:\n",
    "                ax.quiver(\n",
    "                    pos[0],\n",
    "                    pos[1],\n",
    "                    pos[2],\n",
    "                    transformed_axes[0, j] - pos[0],\n",
    "                    transformed_axes[1, j] - pos[1],\n",
    "                    transformed_axes[2, j] - pos[2],\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_title(\"Ultrasound Frame Trajectory and Poses\")\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Set aspect ratio to 'equal'\n",
    "    ax.set_box_aspect(\n",
    "        (np.ptp(positions[:, 0]), np.ptp(positions[:, 1]), np.ptp(positions[:, 2]))\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Assuming 'df' is your DataFrame containing the optical tracker data\n",
    "plot_trajectory_and_poses(df, frame_stride=1, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715a77f-06bb-4c62-8f05-55866851b365",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def reconstruct_ground_truth_volume(df, probe_specs, voxel_size=0.5):\n",
    "    frames = df[\"frame\"].values\n",
    "    positions = df[[\"ot_pos_x\", \"ot_pos_y\", \"ot_pos_z\"]].values\n",
    "    orientations = df[[\"ot_qw\", \"ot_qx\", \"ot_qy\",\"ot_qz\"]].values\n",
    "\n",
    "    # Probe specifications\n",
    "    depth = probe_specs[\"depth\"]  # mm (axial direction)\n",
    "    width = probe_specs[\"width\"]  # mm (lateral direction)\n",
    "    axial_res = probe_specs[\"axial_res\"]  # mm\n",
    "    lateral_res = probe_specs[\"lateral_res\"]  # mm\n",
    "    slice_thickness = probe_specs[\"slice_thickness\"]  # mm (elevational direction)\n",
    "    marker_to_probe_bottom = probe_specs[\"marker_to_probe_bottom\"]  # mm\n",
    "\n",
    "    # Calculate scaling factors\n",
    "    axial_scale = depth / 1000  # mm per pixel in axial direction\n",
    "    lateral_scale = width / 657  # mm per pixel in lateral direction\n",
    "\n",
    "    # Calculate volume bounds\n",
    "    rotations = Rotation.from_quat(orientations[:, [1, 2, 3, 0]])\n",
    "\n",
    "    # Define frame corners in probe coordinates (y: axial, x: lateral, z: elevational)\n",
    "    frame_corners = np.array(\n",
    "        [[0, 0, 0], [width, 0, 0], [0, depth, 0], [width, depth, 0]]\n",
    "    )\n",
    "\n",
    "    # Transform to align with optical tracker coordinates\n",
    "    transform_matrix = np.array(\n",
    "        [\n",
    "            [0, 0, -1],  # Tracker X -> -Probe Z (elevational)\n",
    "            [1, 0, 0],  # Tracker Y -> Probe X (lateral)\n",
    "            [0, -1, 0],  # Tracker Z -> -Probe Y (axial)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    frame_corners = frame_corners @ transform_matrix.T\n",
    "\n",
    "    # Add offset for marker to probe bottom\n",
    "    frame_corners[:, 2] += marker_to_probe_bottom\n",
    "\n",
    "    all_corners = (\n",
    "        np.einsum(\"ijk,lk->ilj\", rotations.as_matrix(), frame_corners)\n",
    "        + positions[:, np.newaxis, :]\n",
    "    )\n",
    "    min_corner = np.min(all_corners.reshape(-1, 3), axis=0)\n",
    "    max_corner = np.max(all_corners.reshape(-1, 3), axis=0)\n",
    "\n",
    "    # Initialize volume\n",
    "    volume_shape = np.ceil((max_corner - min_corner) / voxel_size).astype(int) + 1\n",
    "    volume = np.zeros(volume_shape, dtype=np.float32)\n",
    "    counts = np.zeros(volume_shape, dtype=np.int32)\n",
    "\n",
    "    # Create coordinate grid for frames\n",
    "    x, y = np.meshgrid(np.arange(657) * lateral_scale, np.arange(1000) * axial_scale)\n",
    "    frame_coords = np.stack((x, y, np.zeros_like(x)), axis=-1)\n",
    "\n",
    "    # Transform frame coordinates to align with optical tracker\n",
    "    frame_coords = frame_coords @ transform_matrix.T\n",
    "    frame_coords[:, :, 2] += marker_to_probe_bottom\n",
    "\n",
    "    print(\"Reconstructing ground truth volume...\")\n",
    "    for i, (frame, position, orientation) in enumerate(\n",
    "        tqdm(zip(frames, positions, orientations), total=len(frames))\n",
    "    ):\n",
    "        # Convert frame to grayscale\n",
    "        frame = np.mean(frame, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Transform frame coordinates to world space\n",
    "        rotation = Rotation.from_quat(orientation[[1, 2, 3, 0]]).as_matrix()\n",
    "        world_coords = np.einsum(\"ij,klj->kli\", rotation, frame_coords) + position\n",
    "\n",
    "        # Calculate voxel coordinates\n",
    "        voxel_coords = np.round((world_coords - min_corner) / voxel_size).astype(int)\n",
    "\n",
    "        # Filter out-of-bounds voxels\n",
    "        mask = np.all((voxel_coords >= 0) & (voxel_coords < volume_shape), axis=2)\n",
    "        valid_voxels = voxel_coords[mask]\n",
    "        valid_intensities = frame[mask]\n",
    "\n",
    "        # Update volume and counts\n",
    "        np.add.at(volume, tuple(valid_voxels.T), valid_intensities)\n",
    "        np.add.at(counts, tuple(valid_voxels.T), 1)\n",
    "\n",
    "    # Average intensities and handle zero counts\n",
    "    mask = counts > 0\n",
    "    volume[mask] /= counts[mask]\n",
    "\n",
    "    return volume, min_corner, voxel_size\n",
    "\n",
    "\n",
    "# Probe specifications\n",
    "probe_specs = {\n",
    "    \"depth\": 50,  # mm (axial direction)\n",
    "    \"width\": 38,  # mm (lateral direction, assuming 19 mm on each side of the center)\n",
    "    \"axial_res\": 0.5,  # mm\n",
    "    \"lateral_res\": 2,  # mm\n",
    "    \"slice_thickness\": 5,  # mm (elevational direction)\n",
    "    \"marker_to_probe_bottom\": 10,  # mm (approximate, to be refined)\n",
    "}\n",
    "\n",
    "# Reconstruct ground truth volume\n",
    "ground_truth_volume, volume_origin, voxel_size = reconstruct_ground_truth_volume(\n",
    "    df, probe_specs, voxel_size=0.1  # lower voxel size corresponds to higher resolution\n",
    ")\n",
    "\n",
    "print(f\"Ground truth volume shape: {ground_truth_volume.shape}\")\n",
    "print(f\"Volume origin: {volume_origin}\")\n",
    "print(f\"Voxel size: {voxel_size} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01426dac-4608-4e54-a776-22e19027bbfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Save the ground truth volume\n",
    "np.save(\"ground_truth_volume.npy\", np.flip(ground_truth_volume, axis=0))\n",
    "print(\"Ground truth volume saved as 'ground_truth_volume.npy'\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"volume_origin\": volume_origin.tolist(),\n",
    "    \"voxel_size\": voxel_size,\n",
    "    \"probe_specs\": probe_specs,\n",
    "}\n",
    "np.save(\"ground_truth_volume_metadata.npy\", metadata)\n",
    "print(\"Metadata saved as 'ground_truth_volume_metadata.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b870365-74ae-4fec-9c17-db80b9173ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "ground_truth_volume.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b21bb-a21f-4b75-bd46-a907d58ed705",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Reconstruct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a33077-dfc5-4078-a66b-7767060b6134",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def euler_to_rotation_matrix(euler_angles):\n",
    "    return Rotation.from_euler('xyz', euler_angles).as_matrix()\n",
    "\n",
    "def reconstruct_predicted_volume(frames, predicted_transforms, probe_specs, voxel_size=0.5):\n",
    "    # Extract translations and rotations from predicted transforms\n",
    "    translations = predicted_transforms[:, :3]\n",
    "    rotations = [euler_to_rotation_matrix(euler) for euler in predicted_transforms[:, 3:]]\n",
    "\n",
    "    # Probe specifications\n",
    "    depth = probe_specs[\"depth\"]\n",
    "    width = probe_specs[\"width\"]\n",
    "    marker_to_probe_bottom = probe_specs[\"marker_to_probe_bottom\"]\n",
    "\n",
    "    # Calculate scaling factors\n",
    "    axial_scale = depth / frames.shape[1]\n",
    "    lateral_scale = width / frames.shape[2]\n",
    "\n",
    "    # Calculate volume bounds\n",
    "    frame_corners = np.array([[0, 0, 0], [width, 0, 0], [0, depth, 0], [width, depth, 0]])\n",
    "    transform_matrix = np.array([\n",
    "        [0, 0, -1],\n",
    "        [1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "    ])\n",
    "    frame_corners = frame_corners @ transform_matrix.T\n",
    "    frame_corners[:, 2] += marker_to_probe_bottom\n",
    "\n",
    "    all_corners = np.einsum('ijk,lk->ilj', rotations, frame_corners) + translations[:, np.newaxis, :]\n",
    "    min_corner = np.min(all_corners.reshape(-1, 3), axis=0)\n",
    "    max_corner = np.max(all_corners.reshape(-1, 3), axis=0)\n",
    "\n",
    "    # Initialize volume\n",
    "    volume_shape = np.ceil((max_corner - min_corner) / voxel_size).astype(int) + 1\n",
    "    volume = np.zeros(volume_shape, dtype=np.float32)\n",
    "    counts = np.zeros(volume_shape, dtype=np.int32)\n",
    "\n",
    "    # Create coordinate grid for frames\n",
    "    x, y = np.meshgrid(np.arange(frames.shape[2]) * lateral_scale, np.arange(frames.shape[1]) * axial_scale)\n",
    "    frame_coords = np.stack((x, y, np.zeros_like(x)), axis=-1)\n",
    "    frame_coords = frame_coords @ transform_matrix.T\n",
    "    frame_coords[:, :, 2] += marker_to_probe_bottom\n",
    "\n",
    "    print(\"Reconstructing predicted volume...\")\n",
    "    for i, (frame, translation, rotation) in enumerate(tqdm(zip(frames, translations, rotations), total=len(frames))):\n",
    "        # Ensure frame is 2D (height x width)\n",
    "        if frame.ndim == 3:\n",
    "            frame = np.mean(frame, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Transform frame coordinates to world space\n",
    "        world_coords = np.einsum('ij,klj->kli', rotation, frame_coords) + translation\n",
    "\n",
    "        # Calculate voxel coordinates\n",
    "        voxel_coords = np.round((world_coords - min_corner) / voxel_size).astype(int)\n",
    "\n",
    "        # Filter out-of-bounds voxels\n",
    "        mask = np.all((voxel_coords >= 0) & (voxel_coords < volume_shape), axis=2)\n",
    "        valid_voxels = voxel_coords[mask]\n",
    "        valid_intensities = frame[mask]\n",
    "\n",
    "        # Update volume and counts\n",
    "        np.add.at(volume, tuple(valid_voxels.T), valid_intensities)\n",
    "        np.add.at(counts, tuple(valid_voxels.T), 1)\n",
    "\n",
    "    # Average intensities and handle zero counts\n",
    "    mask = counts > 0\n",
    "    volume[mask] /= counts[mask]\n",
    "\n",
    "    return volume, min_corner, voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd3891-a2b9-4087-bcc8-40eada8fa4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Assuming you have already trained the model and have the validation dataset\n",
    "model.eval()\n",
    "all_frames = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, imu_data, _ in val_loader:\n",
    "        frames = frames.to(device)\n",
    "        imu_data = imu_data.to(device)\n",
    "        outputs = model(frames, imu_data)\n",
    "\n",
    "        all_frames.append(frames.cpu().numpy())\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "all_frames = np.concatenate(all_frames, axis=0)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Reconstruct the predicted volume\n",
    "predicted_volume, volume_origin, voxel_size = reconstruct_predicted_volume(\n",
    "    all_frames[:, 0],  # Use only the first frame of each sequence\n",
    "    all_predictions,  # Use all predictions\n",
    "    probe_specs,\n",
    "    voxel_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"Predicted volume shape: {predicted_volume.shape}\")\n",
    "print(f\"Volume origin: {volume_origin}\")\n",
    "print(f\"Voxel size: {voxel_size} mm\")\n",
    "\n",
    "# Save the predicted volume\n",
    "np.save(\"predicted_volume.npy\", predicted_volume)\n",
    "print(\"Predicted volume saved as 'predicted_volume.npy'\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"volume_origin\": volume_origin.tolist(),\n",
    "    \"voxel_size\": voxel_size,\n",
    "    \"probe_specs\": probe_specs,\n",
    "}\n",
    "np.save(\"predicted_volume_metadata.npy\", metadata)\n",
    "print(\"Metadata saved as 'predicted_volume_metadata.npy'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
